
=====================================================================
==============	flash-page  page allocation/free cost

16M flash pages (64GB),  3-level page-alloc-table.

36 bits flash-address =>  24 bit flash-pages => 1 bit/flash-page,  2^21 bytes bitmap 

PAT "table-1", 3 levels, pgd-pmd-bitmap = 6.6.12

(1)  single-page-alloc/free
16777216 allocs, cost 33503627945 ns, 1996.971842 ns/alloc-one-page

16777216 free, cost 815263559 ns, 48.593495 ns/free-page

(2)  vector-pages-alloc/free

alloc-group = 64 pgs:
262144 allocs at 64 pgs-unit, cost 3004221168 ns, 11460.194275 ns/alloc for 64 pgs,  179 ns / page

alloc-group = 16 pgs:
1048576 allocs at 16 pgs-unit, cost 4295110680 ns, 4096.136742 ns/alloc,  306 ns / page.





==========================================================================
==============        most basic sigsegv handler overhead


at x200 laptop, core2-due,  2.4G,  8GB ram

page fault =>  SIGSEGV handler => search page-cache/ram-cache for target,
     memcpy(), then read protect:

     1 thread,  1 hmem-instance.

avg-lat = 11.52 usec / op

   AccessHybridMemoryWriteThenRead: Thread 0: use rand-seed 5534332, num-pages
   10000000
   AccessHybridMemoryWriteThenRead: hmem found-pages=0, unfound-pages=10000
   AccessHybridMemoryWriteThenRead: hmem found-pages=10000,
   unfound-pages=10000
   AccessHybridMemoryWriteThenRead: Thread 0: sequential-access:
   max-write-latency = 451.664000 usec, max-read-lat = 453.758000 usec
      write-round page faults=10000, read-round page-faults = 10000
      1 threads, 20000 access, 20000 page faults in 230551 usec, 11.527550
      usec/page
      throughput = 86748 access / sec




=============================================================
=====		basic sigsegv fault overhead.

hmem has 16 instances, total page-cache = 512MB

One vaddr-range = 40 GB,  1/2/4 threads perform random access 


NOTE: a read op requires:  mprotect to allow write, copy to the target page, then mprotect to read.
	which is about twice the op as write.
num-thr              1                                    2                                       4
 ( lat=usec )
read-avg-lat        9.70/9.67/9.72/9.69 /9.68           27.34/24.57/28.29/26.71/27.01       35.80/36.41/36.34/34.94/35.39
read-max-lat        100 /147/ 83.63 /82.30              4412/4463/2655/2650/4972/4973     9107/9099/9110/8971/8920/8152/8775
                    80.73                               8596/8577                            3182/3498/3181/3500
read-ops/sec        115259 / 115568/114890/115297       46326/40224                        31344/32612/32193/31292
                    115490                              41498                                31838



write-avg-lat      8.87/8.84/8.82            17.94/20.43/20.31/19.10        25.26/24.76/21.75/25.39
write-max-lat      15.40/12.32              92.73                     92.61 / 94.49/92.11
                   148.7                    5072/4849                3153/3541/3373/3475
write-ops/sec     125930/126262             63468                     46116 / 52548
                   126636                   59603                     44984


50%-read-50%-write    
num-thr             1                       2                             4
avg-lat           9.62/9.46/9.59         22.97/ 23.51/24.27/ 23.7/24.70         29.72/30.59/29.61 / 26.47/30.91
max-lat           147.8/80.75/77.62      1914/1637/6077/6087/1004         4587/4448/4463/10096/10125/10555/10653
                  77.77                  2872/2875/1626/1831               3949/4163/3770/3965 / 2981/3194/3231/2885
op/sec           116103/117076/118225    47910/46097/46913                37288/38503/43068 / 38379
                   118334/116491          48445                               36886



========================================================
======== 		hash-table at ram-cache layer

==================
Hashtable: "test table", 1000000 buckets, 2000000 objs, 
inserts = 2000000, lookups = 4000000, removes = 0, hit = 2000000, miss = 2000000, deepest-collision = 11, collisions = 3998594

2000000 lookup, cost 428879745 ns, 214.439873 ns/lookup




===============================================================
*****		hmem:  sigsegv + page-cache + ram-cache

Now have the basic sigsegv, pagecache(L1) and ram-cache(L2) work.
But L2 cache cannot overflow, so accessed-space should not exceed L2 size.

L1 cache = 10M (2560 pages can be materialized)
L2 cache = 1000 M (can cache 256,000 pages)

virt-space = 10 M pages (40 GB),  access the first 250,000 pages so as not to exceed L2 size.

first write the first 250k pages, then, read them back.
Measure read/write lat (usec), and ops/sec.
work threads access different sections of pages.


1 hmem-instance, 1 worker thread
                 seq                                          random                   random(L1 has only 10 pages)
max-write-lat  103/66/65/133/63                       151/73/67/84/72/
max-read-lat   124/56/52/120/42                       130/61/58/59/61/
avg op lat    6.95/6.92/6.95/ 6.94/6.95               9.27/9.31/9.21/9.29/9.21
total ops/sec   143696/144372/143778/144025/143733    108948/108437/109689/108667/109709

		********* effect of L1 cache size for random access.

Random workloads access random pages spreaded widely, and unprotect the target page before access.
Each uprotect causes a process's vm-area-struct to split, which increases the process's vm-map tree depth.
Each page fault has to traverse the vm-map tree to find the vm-region.
Larger L1 cache means more materialized pages are alive, and more vm-areas in vm-map tree,
so higher overhead at every page fault to search the vm-map tree.

for sequential workload, bigger L1 cache doesn't affect vm-map tree search efficiency because
the contiguous pages are in a same vm-area.  Bigger L1 cache however helps access with strong spatial locality.

                                       1-hem-1-worker-random-access
                      L1 has 2 pages            L1 has 10 pages               L1 has 100 pages
max-write-lat      107/60/108/               103/80/77/81/                    89/136/139/
max-read-lat      106/23/108/                103/42/46/58/                    52/103/125/
avg-op-lat        7.78/7.76/7.79/            7.50/7.46/7.52/7.52/             8.02/7.98/7.99/
total ops/sec     128424/128744/128272       133243/133927/132857/132826      124631/125260/125163

		************************


1 hmem-instance, 2 worker thread
                 seq                                          random
max-wirte-lat   9316/9240/ 240/241/857/791/                  1004/764/299/197/368/426/
max-read-lat    19913/30013/9990/10263/44375/49998/          1921/1334/1474/526/2642/2803/
avg op lat       11.5/12.4/11.88/                            18.72/17.74/17.15/
total ops/sec   86789/ 80413 / 84127                         53953/56924 / 58874


8 hmem-instance, 1 worker thread
                 seq                                          random
max-write-lat   108/66/138/65                            105/98/96/
max-read-lat    89/61/106/51/                            134/61/64/
avg op lat      7.06/7.06/7.10/7.13/                     9.24/9.30/9.32/
total ops/sec   141544/141528/140861/140207              109295/108518/108347/


8 hmem-instance, 2 worker thread
                 seq                                          random
max-wirte-lat    1547/1545/181/258/250/322/2058/2073/3294/3773/      403/369//634/600/4000/3822/1148/1363/
max-read-lat     2716/2708/690/864/993/1067/358/497/10028/866/       989/983/871/1144/645/641/348/406/
avg op lat       15.9/12.8 /12.60/15.99/13.774/                 19.557/20.6/19.68/17.0
total ops/sec    62872/78134/79401/62514 / 72612                 51643/48883/51335/59281

